# ==============================================================
# llama.cpp Environment Variables
# https://github.com/ggml-org/llama.cpp/blob/master/tools/server/README.md
# ==============================================================


# ==============================================================
# llama.cpp select model

LLAMA_ARG_MODEL=data/llm_model/Qwen_Qwen3-0.6B-Q4_K_M.gguf

# ==============================================================
# llama.cpp select multimodal mmproj

# LLAMA_ARG_MMPROJ=data/llm_model/mmproj-google_gemma-3-4b-it-f16.gguf
# LLAMA_ARG_MMPROJ_URL=https://huggingface.co/bartowski/google_gemma-3-4b-it-GGUF/resolve/main/mmproj-google_gemma-3-4b-it-f16.gguf


# ==============================================================
# llama.cpp other params

LLAMA_ARG_CTX_SIZE=2048
LLAMA_ARG_NO_WEBUI=1
LLAMA_LOG_VERBOSITY=0
LLAMA_ARG_N_PARALLEL=1
LLAMA_ARG_N_GPU_LAYERS=-1
LLAMA_ARG_N_PREDICT=2048


# ==============================================================
# llama.cpp disable thinking

LLAMA_ARG_THINK=none
LLAMA_ARG_THINK_BUDGET=0
LLAMA_ARG_JINJA=1

# ==============================================================
# llama.cpp endpoint

LLAMA_ARG_PORT=8080
LLAMA_ARG_HOST=127.0.0.1

HF_TOKEN=""
